{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XHS Data Cleaner\n",
        "\n",
        "Deterministic post-cleaning notebook for `xhs_scraped_data_clean.json`.\n",
        "\n",
        "- Input: `output/json/xhs_scraped_data_clean.json`\n",
        "- Output: `output/json/xhs_scraped_data_cleaned.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "INPUT_PATH = BASE_DIR / \"output/json/xhs_scraped_data_clean.json\"\n",
        "OUTPUT_PATH = BASE_DIR / \"output/json/xhs_scraped_data_cleaned.json\"\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UI_LINE_PATTERNS = [\n",
        "    re.compile(r\"^focus\\s+on$\", re.IGNORECASE),\n",
        "    re.compile(r\"^关注$\"),\n",
        "    re.compile(r\"^edited\\s+on\\s+\\d{4}-\\d{2}-\\d{2}$\", re.IGNORECASE),\n",
        "    re.compile(r\"^there\\s+are\\s+\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^show(?:\\s+\\d+)?\\s+replies?$\", re.IGNORECASE),\n",
        "    re.compile(r\"^praise$\", re.IGNORECASE),\n",
        "    re.compile(r\"^reply$\", re.IGNORECASE),\n",
        "    re.compile(r\"^author$\", re.IGNORECASE),\n",
        "    re.compile(r\"^comment$\", re.IGNORECASE),\n",
        "    re.compile(r\"^the\\s+end$\", re.IGNORECASE),\n",
        "    re.compile(r\"^emoji[\\]\\)]?$\", re.IGNORECASE),\n",
        "    re.compile(r\"^(?:say\\s+something.*|说点什么.*|it'?s\\s*a\\s*wasteland.*|这是一片荒地.*)$\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "DATE_LINE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "SHORT_DATE_LINE = re.compile(r\"^\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "HASHTAG_LINE = re.compile(r\"#\\w+\")\n",
        "LETTER_OR_HAN = re.compile(r\"[A-Za-z\\u4e00-\\u9fff]\")\n",
        "WORD_OR_HAN = re.compile(r\"[A-Za-z]+|[\\u4e00-\\u9fff]+\")\n",
        "PUNCTUATION_MARK = re.compile(r\"[.!?;:。！？]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_line(line: str) -> str:\n",
        "    line = unicodedata.normalize(\"NFKC\", line)\n",
        "    line = line.replace(\"\\u00a0\", \" \")\n",
        "    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "    line = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", line)\n",
        "    line = re.sub(r\"([。！？.!?])\\1{2,}\", r\"\\1\\1\", line)\n",
        "    return line\n",
        "\n",
        "\n",
        "def is_ui_noise(line: str) -> bool:\n",
        "    if not line:\n",
        "        return True\n",
        "    if HASHTAG_LINE.search(line):\n",
        "        return True\n",
        "    return any(pattern.match(line) for pattern in UI_LINE_PATTERNS)\n",
        "\n",
        "\n",
        "def is_counter_or_symbol_noise(line: str) -> bool:\n",
        "    if DATE_LINE.match(line) or SHORT_DATE_LINE.match(line):\n",
        "        return False\n",
        "    if LETTER_OR_HAN.search(line):\n",
        "        return False\n",
        "    compact = line.replace(\" \", \"\")\n",
        "    if not compact:\n",
        "        return True\n",
        "    if len(compact) <= 8 and re.fullmatch(r\"[\\d@QOIl|/\\\\._-]+\", compact):\n",
        "        return True\n",
        "    if re.fullmatch(r\"[\\d\\W_]+\", compact):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_low_information_tail(line: str) -> bool:\n",
        "    if DATE_LINE.match(line) or SHORT_DATE_LINE.match(line):\n",
        "        return False\n",
        "    if is_ui_noise(line) or is_counter_or_symbol_noise(line):\n",
        "        return True\n",
        "\n",
        "    tokens = WORD_OR_HAN.findall(line)\n",
        "    if not tokens:\n",
        "        return True\n",
        "    if PUNCTUATION_MARK.search(line):\n",
        "        return False\n",
        "    return len(tokens) <= 2 and len(line) <= 20\n",
        "\n",
        "\n",
        "def prune_tail_fragments(lines: List[str]) -> List[str]:\n",
        "    idx = len(lines)\n",
        "    run = 0\n",
        "    while idx > 0:\n",
        "        if is_low_information_tail(lines[idx - 1]):\n",
        "            run += 1\n",
        "            idx -= 1\n",
        "            continue\n",
        "        break\n",
        "    return lines[:idx] if run >= 3 else lines\n",
        "\n",
        "\n",
        "def clean_text_block(text: str) -> Dict[str, Any]:\n",
        "    cleaned_lines: List[str] = []\n",
        "    removed_lines: List[str] = []\n",
        "    seen = set()\n",
        "\n",
        "    for raw_line in text.splitlines():\n",
        "        line = normalize_line(raw_line)\n",
        "        if not line:\n",
        "            continue\n",
        "        if is_ui_noise(line) or is_counter_or_symbol_noise(line):\n",
        "            removed_lines.append(line)\n",
        "            continue\n",
        "\n",
        "        key = line.casefold()\n",
        "        if key in seen:\n",
        "            removed_lines.append(line)\n",
        "            continue\n",
        "\n",
        "        seen.add(key)\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    pruned = prune_tail_fragments(cleaned_lines)\n",
        "    if len(pruned) < len(cleaned_lines):\n",
        "        removed_lines.extend(cleaned_lines[len(pruned):])\n",
        "\n",
        "    return {\n",
        "        \"enhanced_cleaned\": \"\\n\".join(pruned).strip(),\n",
        "        \"removed_line_count\": len(removed_lines),\n",
        "        \"removed_lines_sample\": removed_lines[:8],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_dataset(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    categories: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    post_count = 0\n",
        "    removed_line_total = 0\n",
        "\n",
        "    for category, posts in data.items():\n",
        "        transformed_posts: List[Dict[str, Any]] = []\n",
        "        for post in posts:\n",
        "            source_cleaned = str(post.get(\"cleaned\", \"\"))\n",
        "            result = clean_text_block(source_cleaned)\n",
        "            transformed_posts.append(\n",
        "                {\n",
        "                    \"filename\": post.get(\"filename\"),\n",
        "                    \"source_cleaned\": source_cleaned,\n",
        "                    \"enhanced_cleaned\": result[\"enhanced_cleaned\"],\n",
        "                    \"removed_line_count\": result[\"removed_line_count\"],\n",
        "                    \"removed_lines_sample\": result[\"removed_lines_sample\"],\n",
        "                }\n",
        "            )\n",
        "            post_count += 1\n",
        "            removed_line_total += result[\"removed_line_count\"]\n",
        "\n",
        "        categories[category] = transformed_posts\n",
        "\n",
        "    return {\n",
        "        \"cleaner\": \"xhs_data_cleaner_v1_notebook\",\n",
        "        \"input_schema\": \"{category: [{filename, cleaned, ...}]}\",\n",
        "        \"output_schema\": \"{category: [{filename, source_cleaned, enhanced_cleaned, ...}]}\",\n",
        "        \"stats\": {\n",
        "            \"categories\": len(categories),\n",
        "            \"posts\": post_count,\n",
        "            \"removed_lines_total\": removed_line_total,\n",
        "        },\n",
        "        \"categories\": categories,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_cleaner(input_path: Path = INPUT_PATH, output_path: Path = OUTPUT_PATH) -> Dict[str, Any]:\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    transformed = transform_dataset(data)\n",
        "\n",
        "    with output_path.open(\"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(transformed, outfile, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = run_cleaner()\n",
        "print(f\"Cleaned output saved to: {OUTPUT_PATH}\")\n",
        "print(result[\"stats\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
