{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XHS Data Cleaner (Pattern-Driven)\n",
        "\n",
        "Noise-removal notebook for `xhs_scraped_data_clean.json`.\n",
        "\n",
        "- Learns recurring low-value OCR lines from the input dataset\n",
        "- Applies deterministic UI/comment-tail cleanup\n",
        "- Preserves main post content for downstream labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 1: Imports, file paths, and global regex patterns used by cleaner\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Set\n",
        "\n",
        "# Notebook expects to run from server/scripts/xhsScrape\n",
        "BASE_DIR = Path.cwd()\n",
        "INPUT_PATH = BASE_DIR / \"xhs_scraped_data_clean.json\"\n",
        "OUTPUT_PATH = BASE_DIR / \"xhs_scraped_data_cleaned.json\"\n",
        "\n",
        "# Static UI/noise patterns commonly seen in OCR captures\n",
        "UI_PATTERNS = [\n",
        "    re.compile(r\"^focus\\s+on$\", re.IGNORECASE),\n",
        "    re.compile(r\"^关注$\"),\n",
        "    re.compile(r\"^edited\\s+on\\s+\\d{4}-\\d{2}-\\d{2}$\", re.IGNORECASE),\n",
        "    re.compile(r\"^\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^there\\s+are\\s+\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^show(?:\\s+\\d+)?\\s+replies?$\", re.IGNORECASE),\n",
        "    re.compile(r\"^reply$\", re.IGNORECASE),\n",
        "    re.compile(r\"^author$\", re.IGNORECASE),\n",
        "    re.compile(r\"^comment$\", re.IGNORECASE),\n",
        "    re.compile(r\"^praise$\", re.IGNORECASE),\n",
        "    re.compile(r\"^(?:say\\s+something.*|说点什么.*|it'?s\\s*a\\s*wasteland.*|这是一片荒地.*)$\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "DATE_LINE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "SHORT_DATE_LINE = re.compile(r\"^\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "HASHTAG = re.compile(r\"#\\w+\")\n",
        "NOISE_CODE = re.compile(r\"^[A-Za-z0-9]{6,}$\")\n",
        "NON_CONTENT = re.compile(r\"^[\\d\\s@QOIl|/\\\\._()><-]+$\")\n",
        "WORD_OR_HAN = re.compile(r\"[A-Za-z]+|[\\u4e00-\\u9fff]+\")\n",
        "# Markers that usually indicate comment/thread tail (not main post body)\n",
        "COMMENT_MARKERS = [\n",
        "    re.compile(r\"comments?\\s+in\\s+total\", re.IGNORECASE),\n",
        "    re.compile(r\"show(?:\\s+\\d+)?\\s+replies?\", re.IGNORECASE),\n",
        "    re.compile(r\"\\bauthor\\b\", re.IGNORECASE),\n",
        "    re.compile(r\"\\breply\\b\", re.IGNORECASE),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 2: Line normalization + noise classification + dynamic noise learning\n",
        "def normalize_line(line: str) -> str:\n",
        "    line = unicodedata.normalize(\"NFKC\", line)\n",
        "    line = line.replace(\"\\u00a0\", \" \")\n",
        "    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "    line = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", line)\n",
        "    return line\n",
        "\n",
        "\n",
        "def line_is_ui_noise(line: str, learned_noise: Set[str]) -> bool:\n",
        "    if not line:\n",
        "        return True\n",
        "    lowered = line.casefold()\n",
        "    if lowered in learned_noise:\n",
        "        return True\n",
        "    if HASHTAG.search(line):\n",
        "        return True\n",
        "    if NOISE_CODE.fullmatch(line) and not WORD_OR_HAN.search(line.lower()):\n",
        "        return True\n",
        "    if NON_CONTENT.fullmatch(line):\n",
        "        return True\n",
        "    if any(pat.match(line) for pat in UI_PATTERNS):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def looks_like_comment_tail_start(line: str) -> bool:\n",
        "    if any(pat.search(line) for pat in COMMENT_MARKERS):\n",
        "        return True\n",
        "    if DATE_LINE.match(line) or SHORT_DATE_LINE.match(line):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Learns repeated low-value lines from the current dataset itself\n",
        "def learn_noise_patterns(data: Dict[str, Any]) -> Set[str]:\n",
        "    counter: Counter[str] = Counter()\n",
        "    post_count = 0\n",
        "\n",
        "    for posts in data.values():\n",
        "        for post in posts:\n",
        "            post_count += 1\n",
        "            source = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            seen_local = set()\n",
        "            for raw_line in source.splitlines():\n",
        "                line = normalize_line(raw_line)\n",
        "                if not line:\n",
        "                    continue\n",
        "                key = line.casefold()\n",
        "                if key in seen_local:\n",
        "                    continue\n",
        "                seen_local.add(key)\n",
        "                counter[key] += 1\n",
        "\n",
        "    learned = set()\n",
        "    if post_count == 0:\n",
        "        return learned\n",
        "\n",
        "    threshold = max(2, int(post_count * 0.4))\n",
        "    for line_key, freq in counter.items():\n",
        "        if freq < threshold:\n",
        "            continue\n",
        "\n",
        "        tokens = WORD_OR_HAN.findall(line_key)\n",
        "        if len(tokens) <= 3 and len(line_key) <= 40:\n",
        "            if any(marker.search(line_key) for marker in COMMENT_MARKERS) or NON_CONTENT.fullmatch(line_key):\n",
        "                learned.add(line_key)\n",
        "\n",
        "    return learned\n",
        "\n",
        "\n",
        "# Applies deterministic cleanup to one OCR text block\n",
        "def clean_text_block(text: str, learned_noise: Set[str]) -> Dict[str, Any]:\n",
        "    lines = [normalize_line(line) for line in text.splitlines()]\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    cleaned: List[str] = []\n",
        "    removed: List[str] = []\n",
        "    seen = set()\n",
        "\n",
        "    comment_tail_started = False\n",
        "    for line in lines:\n",
        "        if line_is_ui_noise(line, learned_noise):\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if looks_like_comment_tail_start(line):\n",
        "            comment_tail_started = True\n",
        "\n",
        "        if comment_tail_started:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        key = line.casefold()\n",
        "        if key in seen:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if len(WORD_OR_HAN.findall(line)) <= 1 and len(line) < 6:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        seen.add(key)\n",
        "        cleaned.append(line)\n",
        "\n",
        "    return {\n",
        "        \"enhanced_cleaned\": \"\\n\".join(cleaned).strip(),\n",
        "        \"removed_line_count\": len(removed),\n",
        "        \"removed_lines_sample\": removed[:12],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 3: Dataset-level transformation and summary statistics\n",
        "def transform_dataset(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    learned_noise = learn_noise_patterns(data)\n",
        "\n",
        "    categories: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    post_count = 0\n",
        "    removed_total = 0\n",
        "\n",
        "    for category, posts in data.items():\n",
        "        transformed_posts: List[Dict[str, Any]] = []\n",
        "        for post in posts:\n",
        "            source_text = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            result = clean_text_block(source_text, learned_noise=learned_noise)\n",
        "\n",
        "            transformed_posts.append({\n",
        "                \"filename\": post.get(\"filename\"),\n",
        "                \"source_cleaned\": source_text,\n",
        "                \"enhanced_cleaned\": result[\"enhanced_cleaned\"],\n",
        "                \"removed_line_count\": result[\"removed_line_count\"],\n",
        "                \"removed_lines_sample\": result[\"removed_lines_sample\"],\n",
        "            })\n",
        "\n",
        "            post_count += 1\n",
        "            removed_total += result[\"removed_line_count\"]\n",
        "\n",
        "        categories[category] = transformed_posts\n",
        "\n",
        "    return {\n",
        "        \"cleaner\": \"xhs_data_cleaner_v2_pattern_notebook\",\n",
        "        \"input_path\": str(INPUT_PATH),\n",
        "        \"output_path\": str(OUTPUT_PATH),\n",
        "        \"stats\": {\n",
        "            \"categories\": len(categories),\n",
        "            \"posts\": post_count,\n",
        "            \"removed_lines_total\": removed_total,\n",
        "            \"learned_noise_count\": len(learned_noise),\n",
        "            \"learned_noise_sample\": sorted(list(learned_noise))[:12],\n",
        "        },\n",
        "        \"categories\": categories,\n",
        "    }\n",
        "\n",
        "\n",
        "# Orchestrates read -> clean -> write pipeline\n",
        "def run_cleaner(input_path: Path = INPUT_PATH, output_path: Path = OUTPUT_PATH) -> Dict[str, Any]:\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    transformed = transform_dataset(data)\n",
        "\n",
        "    with output_path.open(\"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(transformed, outfile, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 4: Execute cleaner and print quick QA preview\n",
        "result = run_cleaner()\n",
        "print(f\"Cleaned output saved to: {OUTPUT_PATH}\")\n",
        "print(result[\"stats\"])\n",
        "\n",
        "for category, posts in result[\"categories\"].items():\n",
        "    safe_category = category.encode('ascii', 'ignore').decode('ascii') or '<non-ascii-category>'\n",
        "    print(f\"\\n[{safe_category}]\")\n",
        "    for post in posts[:2]:\n",
        "        snippet = post['enhanced_cleaned'][:300].encode('ascii', 'ignore').decode('ascii')\n",
        "        print(f\"- {post['filename']}\\n{snippet}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
