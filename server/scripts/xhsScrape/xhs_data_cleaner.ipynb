{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XHS Data Cleaner (Pattern-Driven)\n",
        "\n",
        "Noise-removal notebook for `xhs_scraped_data_clean.json`.\n",
        "\n",
        "- Learns recurring low-value OCR lines from the input dataset\n",
        "- Applies deterministic UI/comment-tail cleanup\n",
        "- Preserves main post content for downstream labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f840b6ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 1: Imports, file paths, and global regex patterns used by cleaner\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Set\n",
        "\n",
        "# Notebook expects to run from server/scripts/xhsScrape\n",
        "BASE_DIR = Path.cwd()\n",
        "INPUT_PATH = BASE_DIR / \"xhs_scraped_data_clean.json\"\n",
        "OUTPUT_PATH = BASE_DIR / \"xhs_scraped_data_cleaned.json\"\n",
        "CSV_PATH = BASE_DIR / \"xhs_scraped_data_clean.csv\"\n",
        "\n",
        "# Static UI/noise patterns commonly seen in OCR captures\n",
        "UI_PATTERNS = [\n",
        "    re.compile(r\"^focus\\s+on$\", re.IGNORECASE),\n",
        "    re.compile(r\"^关注$\"),\n",
        "    re.compile(r\"^edited\\s+on\\s+\\d{4}-\\d{2}-\\d{2}$\", re.IGNORECASE),\n",
        "    re.compile(r\"^\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^there\\s+are\\s+\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^show(?:\\s+\\d+)?\\s+replies?$\", re.IGNORECASE),\n",
        "    re.compile(r\"^reply$\", re.IGNORECASE),\n",
        "    re.compile(r\"^author$\", re.IGNORECASE),\n",
        "    re.compile(r\"^comment$\", re.IGNORECASE),\n",
        "    re.compile(r\"^praise$\", re.IGNORECASE),\n",
        "    re.compile(r\"^(?:say\\s+something.*|说点什么.*|it'?s\\s*a\\s*wasteland.*|这是一片荒地.*)$\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "DATE_LINE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "SHORT_DATE_LINE = re.compile(r\"^\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "HASHTAG = re.compile(r\"#\\w+\")\n",
        "NOISE_CODE = re.compile(r\"^[A-Za-z0-9]{6,}$\")\n",
        "NON_CONTENT = re.compile(r\"^[\\d\\s@QOIl|/\\\\._()><-]+$\")\n",
        "SPECIAL_CHARS = re.compile(r\"[^\\w\\s\\u4e00-\\u9fff]\")\n",
        "WORD_OR_HAN = re.compile(r\"[A-Za-z]+|[\\u4e00-\\u9fff]+\")\n",
        "# Markers that usually indicate comment/thread tail (not main post body)\n",
        "COMMENT_MARKERS = [\n",
        "    re.compile(r\"comments?\\s+in\\s+total\", re.IGNORECASE),\n",
        "    re.compile(r\"show(?:\\s+\\d+)?\\s+replies?\", re.IGNORECASE),\n",
        "    re.compile(r\"\\bauthor\\b\", re.IGNORECASE),\n",
        "    re.compile(r\"\\breply\\b\", re.IGNORECASE),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c7de56aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 2: Line normalization + noise classification + dynamic noise learning\n",
        "def normalize_line(line: str) -> str:\n",
        "    line = unicodedata.normalize(\"NFKC\", line)\n",
        "    line = line.replace(\"/n\", \" \")\n",
        "    line = line.replace(\"\\u00a0\", \" \")\n",
        "    line = SPECIAL_CHARS.sub(\" \", line)\n",
        "    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "    return line\n",
        "\n",
        "\n",
        "def line_is_ui_noise(line: str, learned_noise: Set[str]) -> bool:\n",
        "    if not line:\n",
        "        return True\n",
        "    lowered = line.casefold()\n",
        "    if lowered in learned_noise:\n",
        "        return True\n",
        "    if HASHTAG.search(line):\n",
        "        return True\n",
        "    if NOISE_CODE.fullmatch(line) and not WORD_OR_HAN.search(line.lower()):\n",
        "        return True\n",
        "    if NON_CONTENT.fullmatch(line):\n",
        "        return True\n",
        "    if any(pat.match(line) for pat in UI_PATTERNS):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def looks_like_comment_tail_start(line: str) -> bool:\n",
        "    if any(pat.search(line) for pat in COMMENT_MARKERS):\n",
        "        return True\n",
        "    if DATE_LINE.match(line) or SHORT_DATE_LINE.match(line):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Learns repeated low-value lines from the current dataset itself\n",
        "def learn_noise_patterns(data: Dict[str, Any]) -> Set[str]:\n",
        "    counter: Counter[str] = Counter()\n",
        "    post_count = 0\n",
        "\n",
        "    for posts in data.values():\n",
        "        for post in posts:\n",
        "            post_count += 1\n",
        "            source = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            seen_local = set()\n",
        "            for raw_line in source.splitlines():\n",
        "                line = normalize_line(raw_line)\n",
        "                if not line:\n",
        "                    continue\n",
        "                key = line.casefold()\n",
        "                if key in seen_local:\n",
        "                    continue\n",
        "                seen_local.add(key)\n",
        "                counter[key] += 1\n",
        "\n",
        "    learned = set()\n",
        "    if post_count == 0:\n",
        "        return learned\n",
        "\n",
        "    threshold = max(2, int(post_count * 0.4))\n",
        "    for line_key, freq in counter.items():\n",
        "        if freq < threshold:\n",
        "            continue\n",
        "\n",
        "        tokens = WORD_OR_HAN.findall(line_key)\n",
        "        if len(tokens) <= 3 and len(line_key) <= 40:\n",
        "            if any(marker.search(line_key) for marker in COMMENT_MARKERS) or NON_CONTENT.fullmatch(line_key):\n",
        "                learned.add(line_key)\n",
        "\n",
        "    return learned\n",
        "\n",
        "\n",
        "# Applies deterministic cleanup to one OCR text block\n",
        "def clean_text_block(text: str, learned_noise: Set[str]) -> Dict[str, Any]:\n",
        "    text = text.replace(\"/n\", \"\\n\")\n",
        "    source_lines = text.splitlines()\n",
        "    normalized_source = [normalize_line(line) for line in source_lines]\n",
        "    focus_index = next((idx for idx, line in enumerate(normalized_source) if \"focus on\" in line.casefold()), None)\n",
        "    if focus_index is not None:\n",
        "        source_lines = source_lines[focus_index + 1:]\n",
        "\n",
        "    lines = [normalize_line(line) for line in source_lines]\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    cleaned: List[str] = []\n",
        "    removed: List[str] = []\n",
        "    seen = set()\n",
        "\n",
        "    comment_tail_started = False\n",
        "    for line in lines:\n",
        "        if line_is_ui_noise(line, learned_noise):\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if looks_like_comment_tail_start(line):\n",
        "            comment_tail_started = True\n",
        "\n",
        "        if comment_tail_started:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        key = line.casefold()\n",
        "        if key in seen:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if len(WORD_OR_HAN.findall(line)) <= 1 and len(line) < 6:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        seen.add(key)\n",
        "        cleaned.append(line)\n",
        "\n",
        "    return {\n",
        "        \"enhanced_cleaned\": re.sub(r\"\\s+\", \" \", \" \".join(cleaned)).strip(),\n",
        "        \"removed_line_count\": len(removed),\n",
        "        \"removed_lines_sample\": removed[:12],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c3ab1a33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 3: JSON -> CSV transform, then dataset cleaning + summary\n",
        "def flatten_posts_for_csv(data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "    rows: List[Dict[str, str]] = []\n",
        "    for category, posts in data.items():\n",
        "        for idx, post in enumerate(posts, start=1):\n",
        "            if not isinstance(post, dict):\n",
        "                continue\n",
        "            rows.append({\n",
        "                \"category\": str(category),\n",
        "                \"post_index\": str(idx),\n",
        "                \"filename\": str(post.get(\"filename\") or \"\"),\n",
        "                \"window_title\": str(post.get(\"window_title\") or \"\"),\n",
        "                \"scraped_at\": str(post.get(\"scraped_at\") or \"\"),\n",
        "                \"raw\": str(post.get(\"raw\") or \"\"),\n",
        "                \"cleaned\": str(post.get(\"cleaned\") or \"\"),\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def write_csv(rows: List[Dict[str, str]], csv_path: Path = CSV_PATH) -> Path:\n",
        "    fields = [\"category\", \"post_index\", \"filename\", \"window_title\", \"scraped_at\", \"raw\", \"cleaned\"]\n",
        "    with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fields)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    return csv_path\n",
        "\n",
        "\n",
        "def transform_dataset(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    learned_noise = learn_noise_patterns(data)\n",
        "\n",
        "    categories: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    post_count = 0\n",
        "    removed_total = 0\n",
        "\n",
        "    for category, posts in data.items():\n",
        "        transformed_posts: List[Dict[str, Any]] = []\n",
        "        for post in posts:\n",
        "            source_text = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            result = clean_text_block(source_text, learned_noise=learned_noise)\n",
        "\n",
        "            transformed_posts.append({\n",
        "                \"filename\": post.get(\"filename\"),\n",
        "                \"window_title\": post.get(\"window_title\"),\n",
        "                \"scraped_at\": post.get(\"scraped_at\"),\n",
        "                \"enhanced_cleaned\": result[\"enhanced_cleaned\"],\n",
        "            })\n",
        "\n",
        "            post_count += 1\n",
        "            removed_total += result[\"removed_line_count\"]\n",
        "\n",
        "        categories[category] = transformed_posts\n",
        "\n",
        "    return {\n",
        "        \"cleaner\": \"xhs_data_cleaner_v2_pattern_notebook\",\n",
        "        \"input_path\": str(INPUT_PATH),\n",
        "        \"output_path\": str(OUTPUT_PATH),\n",
        "        \"stats\": {\n",
        "            \"categories\": len(categories),\n",
        "            \"posts\": post_count,\n",
        "            \"removed_lines_total\": removed_total,\n",
        "            \"learned_noise_count\": len(learned_noise),\n",
        "            \"learned_noise_sample\": sorted(list(learned_noise))[:12],\n",
        "        },\n",
        "        \"categories\": categories,\n",
        "    }\n",
        "\n",
        "\n",
        "# Orchestrates read -> clean -> write pipeline\n",
        "def run_cleaner(input_path: Path = INPUT_PATH, output_path: Path = OUTPUT_PATH, csv_path: Path = CSV_PATH) -> Dict[str, Any]:\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    csv_rows = flatten_posts_for_csv(data)\n",
        "    write_csv(csv_rows, csv_path=csv_path)\n",
        "\n",
        "    transformed = transform_dataset(data)\n",
        "    transformed[\"csv_path\"] = str(csv_path)\n",
        "    transformed[\"stats\"][\"csv_rows\"] = len(csv_rows)\n",
        "\n",
        "    with output_path.open(\"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(transformed, outfile, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "89eb2e8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV output saved to: d:\\Ash Stuff\\Coding\\KitaHack 2026\\donki-wonki\\server\\scripts\\xhsScrape\\xhs_scraped_data_clean.csv\n",
            "Cleaned output saved to: d:\\Ash Stuff\\Coding\\KitaHack 2026\\donki-wonki\\server\\scripts\\xhsScrape\\xhs_scraped_data_cleaned.json\n",
            "{'categories': 14, 'posts': 127, 'removed_lines_total': 2187, 'learned_noise_count': 2, 'learned_noise_sample': ['author', 'reply'], 'csv_rows': 127}\n"
          ]
        }
      ],
      "source": [
        "# Segment 4: Execute cleaner and print quick QA preview\n",
        "result = run_cleaner()\n",
        "print(f\"CSV output saved to: {CSV_PATH}\")\n",
        "print(f\"Cleaned output saved to: {OUTPUT_PATH}\")\n",
        "print(result[\"stats\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
