{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XHS Data Cleaner (Pattern-Driven)\n",
        "\n",
        "Noise-removal notebook for `xhs_scraped_data_clean.json`.\n",
        "\n",
        "- Learns recurring low-value OCR lines from the input dataset\n",
        "- Applies deterministic UI/comment-tail cleanup\n",
        "- Preserves main post content for downstream labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f840b6ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 1: Imports, file paths, and global regex patterns used by cleaner\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Set\n",
        "\n",
        "# Notebook expects to run from server/scripts/xhsScrape\n",
        "BASE_DIR = Path.cwd()\n",
        "INPUT_PATH = BASE_DIR / \"xhs_scraped_data_clean.json\"\n",
        "OUTPUT_PATH = BASE_DIR / \"xhs_scraped_data_cleaned.json\"\n",
        "CSV_PATH = BASE_DIR / \"xhs_scraped_data_clean.csv\"\n",
        "\n",
        "# Static UI/noise patterns commonly seen in OCR captures\n",
        "UI_PATTERNS = [\n",
        "    re.compile(r\"^focus\\s+on$\", re.IGNORECASE),\n",
        "    re.compile(r\"^关注$\"),\n",
        "    re.compile(r\"^edited\\s+on\\s+\\d{4}-\\d{2}-\\d{2}$\", re.IGNORECASE),\n",
        "    re.compile(r\"^\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^there\\s+are\\s+\\d+\\s+comments?\\s+in\\s+total$\", re.IGNORECASE),\n",
        "    re.compile(r\"^show(?:\\s+\\d+)?\\s+replies?$\", re.IGNORECASE),\n",
        "    re.compile(r\"^reply$\", re.IGNORECASE),\n",
        "    re.compile(r\"^author$\", re.IGNORECASE),\n",
        "    re.compile(r\"^comment$\", re.IGNORECASE),\n",
        "    re.compile(r\"^praise$\", re.IGNORECASE),\n",
        "    re.compile(r\"^(?:say\\s+something.*|说点什么.*|it'?s\\s*a\\s*wasteland.*|这是一片荒地.*)$\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "DATE_LINE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "SHORT_DATE_LINE = re.compile(r\"^\\d{2}-\\d{2}(?:\\s+[A-Za-z]+)?$\", re.IGNORECASE)\n",
        "HASHTAG = re.compile(r\"#\\w+\")\n",
        "NOISE_CODE = re.compile(r\"^[A-Za-z0-9]{6,}$\")\n",
        "NON_CONTENT = re.compile(r\"^[\\d\\s@QOIl|/\\\\._()><-]+$\")\n",
        "SPECIAL_CHARS = re.compile(r\"[^\\w\\s\\u4e00-\\u9fff]\")\n",
        "WORD_OR_HAN = re.compile(r\"[A-Za-z]+|[\\u4e00-\\u9fff]+\")\n",
        "# Markers that usually indicate comment/thread tail (not main post body)\n",
        "COMMENT_MARKERS = [\n",
        "    re.compile(r\"comments?\\s+in\\s+total\", re.IGNORECASE),\n",
        "    re.compile(r\"show(?:\\s+\\d+)?\\s+replies?\", re.IGNORECASE),\n",
        "    re.compile(r\"\\bauthor\\b\", re.IGNORECASE),\n",
        "    re.compile(r\"\\breply\\b\", re.IGNORECASE),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c7de56aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 2: Line normalization + noise classification + dynamic noise learning\n",
        "def normalize_line(line: str) -> str:\n",
        "    line = unicodedata.normalize(\"NFKC\", line)\n",
        "    line = line.replace(\"/n\", \" \")\n",
        "    line = line.replace(\"\\u00a0\", \" \")\n",
        "    line = SPECIAL_CHARS.sub(\" \", line)\n",
        "    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "    return line\n",
        "\n",
        "\n",
        "def line_is_ui_noise(line: str, learned_noise: Set[str]) -> bool:\n",
        "    if not line:\n",
        "        return True\n",
        "    lowered = line.casefold()\n",
        "    if lowered in learned_noise:\n",
        "        return True\n",
        "    if HASHTAG.search(line):\n",
        "        return True\n",
        "    if NOISE_CODE.fullmatch(line) and not WORD_OR_HAN.search(line.lower()):\n",
        "        return True\n",
        "    if NON_CONTENT.fullmatch(line):\n",
        "        return True\n",
        "    if any(pat.match(line) for pat in UI_PATTERNS):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def looks_like_comment_tail_start(line: str) -> bool:\n",
        "    if any(pat.search(line) for pat in COMMENT_MARKERS):\n",
        "        return True\n",
        "    if DATE_LINE.match(line) or SHORT_DATE_LINE.match(line):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Learns repeated low-value lines from the current dataset itself\n",
        "def learn_noise_patterns(data: Dict[str, Any]) -> Set[str]:\n",
        "    counter: Counter[str] = Counter()\n",
        "    post_count = 0\n",
        "\n",
        "    for posts in data.values():\n",
        "        for post in posts:\n",
        "            post_count += 1\n",
        "            source = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            seen_local = set()\n",
        "            for raw_line in source.splitlines():\n",
        "                line = normalize_line(raw_line)\n",
        "                if not line:\n",
        "                    continue\n",
        "                key = line.casefold()\n",
        "                if key in seen_local:\n",
        "                    continue\n",
        "                seen_local.add(key)\n",
        "                counter[key] += 1\n",
        "\n",
        "    learned = set()\n",
        "    if post_count == 0:\n",
        "        return learned\n",
        "\n",
        "    threshold = max(2, int(post_count * 0.4))\n",
        "    for line_key, freq in counter.items():\n",
        "        if freq < threshold:\n",
        "            continue\n",
        "\n",
        "        tokens = WORD_OR_HAN.findall(line_key)\n",
        "        if len(tokens) <= 3 and len(line_key) <= 40:\n",
        "            if any(marker.search(line_key) for marker in COMMENT_MARKERS) or NON_CONTENT.fullmatch(line_key):\n",
        "                learned.add(line_key)\n",
        "\n",
        "    return learned\n",
        "\n",
        "\n",
        "# Applies deterministic cleanup to one OCR text block\n",
        "def clean_text_block(text: str, learned_noise: Set[str]) -> Dict[str, Any]:\n",
        "    text = text.replace(\"/n\", \"\\n\")\n",
        "    source_lines = text.splitlines()\n",
        "    normalized_source = [normalize_line(line) for line in source_lines]\n",
        "    focus_index = next((idx for idx, line in enumerate(normalized_source) if \"focus on\" in line.casefold()), None)\n",
        "    if focus_index is not None:\n",
        "        source_lines = source_lines[focus_index + 1:]\n",
        "\n",
        "    lines = [normalize_line(line) for line in source_lines]\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    cleaned: List[str] = []\n",
        "    removed: List[str] = []\n",
        "    seen = set()\n",
        "\n",
        "    comment_tail_started = False\n",
        "    for line in lines:\n",
        "        if line_is_ui_noise(line, learned_noise):\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if looks_like_comment_tail_start(line):\n",
        "            comment_tail_started = True\n",
        "\n",
        "        if comment_tail_started:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        key = line.casefold()\n",
        "        if key in seen:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        if len(WORD_OR_HAN.findall(line)) <= 1 and len(line) < 6:\n",
        "            removed.append(line)\n",
        "            continue\n",
        "\n",
        "        seen.add(key)\n",
        "        cleaned.append(line)\n",
        "\n",
        "    return {\n",
        "        \"enhanced_cleaned\": re.sub(r\"\\s+\", \" \", \" \".join(cleaned)).strip(),\n",
        "        \"removed_line_count\": len(removed),\n",
        "        \"removed_lines_sample\": removed[:12],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c3ab1a33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment 3: JSON -> CSV transform, then dataset cleaning + summary\n",
        "def flatten_posts_for_csv(data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "    rows: List[Dict[str, str]] = []\n",
        "    for category, posts in data.items():\n",
        "        for idx, post in enumerate(posts, start=1):\n",
        "            if not isinstance(post, dict):\n",
        "                continue\n",
        "            rows.append({\n",
        "                \"category\": str(category),\n",
        "                \"post_index\": str(idx),\n",
        "                \"filename\": str(post.get(\"filename\") or \"\"),\n",
        "                \"window_title\": str(post.get(\"window_title\") or \"\"),\n",
        "                \"scraped_at\": str(post.get(\"scraped_at\") or \"\"),\n",
        "                \"raw\": str(post.get(\"raw\") or \"\"),\n",
        "                \"cleaned\": str(post.get(\"cleaned\") or \"\"),\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def write_csv(rows: List[Dict[str, str]], csv_path: Path = CSV_PATH) -> Path:\n",
        "    fields = [\"category\", \"post_index\", \"filename\", \"window_title\", \"scraped_at\", \"raw\", \"cleaned\"]\n",
        "    with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fields)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    return csv_path\n",
        "\n",
        "\n",
        "def transform_dataset(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    learned_noise = learn_noise_patterns(data)\n",
        "\n",
        "    categories: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    post_count = 0\n",
        "    removed_total = 0\n",
        "\n",
        "    for category, posts in data.items():\n",
        "        transformed_posts: List[Dict[str, Any]] = []\n",
        "        for post in posts:\n",
        "            source_text = str(post.get(\"cleaned\") or post.get(\"raw\") or \"\")\n",
        "            result = clean_text_block(source_text, learned_noise=learned_noise)\n",
        "\n",
        "            transformed_posts.append({\n",
        "                \"filename\": post.get(\"filename\"),\n",
        "                \"window_title\": post.get(\"window_title\"),\n",
        "                \"scraped_at\": post.get(\"scraped_at\"),\n",
        "                \"enhanced_cleaned\": result[\"enhanced_cleaned\"],\n",
        "            })\n",
        "\n",
        "            post_count += 1\n",
        "            removed_total += result[\"removed_line_count\"]\n",
        "\n",
        "        categories[category] = transformed_posts\n",
        "\n",
        "    return {\n",
        "        \"cleaner\": \"xhs_data_cleaner_v2_pattern_notebook\",\n",
        "        \"input_path\": str(INPUT_PATH),\n",
        "        \"output_path\": str(OUTPUT_PATH),\n",
        "        \"stats\": {\n",
        "            \"categories\": len(categories),\n",
        "            \"posts\": post_count,\n",
        "            \"removed_lines_total\": removed_total,\n",
        "            \"learned_noise_count\": len(learned_noise),\n",
        "            \"learned_noise_sample\": sorted(list(learned_noise))[:12],\n",
        "        },\n",
        "        \"categories\": categories,\n",
        "    }\n",
        "\n",
        "\n",
        "# Orchestrates read -> clean -> write pipeline\n",
        "def run_cleaner(input_path: Path = INPUT_PATH, output_path: Path = OUTPUT_PATH, csv_path: Path = CSV_PATH) -> Dict[str, Any]:\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    csv_rows = flatten_posts_for_csv(data)\n",
        "    write_csv(csv_rows, csv_path=csv_path)\n",
        "\n",
        "    transformed = transform_dataset(data)\n",
        "    transformed[\"csv_path\"] = str(csv_path)\n",
        "    transformed[\"stats\"][\"csv_rows\"] = len(csv_rows)\n",
        "\n",
        "    with output_path.open(\"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(transformed, outfile, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89eb2e8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV output saved to: d:\\Ash Stuff\\Coding\\KitaHack 2026\\donki-wonki\\server\\scripts\\xhsScrape\\xhs_scraped_data_clean.csv\n",
            "Cleaned output saved to: d:\\Ash Stuff\\Coding\\KitaHack 2026\\donki-wonki\\server\\scripts\\xhsScrape\\xhs_scraped_data_cleaned.json\n",
            "{'categories': 11, 'posts': 101, 'removed_lines_total': 2589, 'learned_noise_count': 2, 'learned_noise_sample': ['author', 'reply'], 'csv_rows': 101}\n",
            "\n",
            "[LRT KelanajayaLine, -  - Google Chrome]\n",
            "- post_1.png\n",
            "There is a problem with LRT Kelana *Restored to normalx Edited on February Gth in Malaysia\n",
            "\n",
            "- post_2.png\n",
            "MYLRT Kelanajayaline Announcement: Trains Reduce Speed Rapid Rail has issued a media statement confirming that the Kelana Jaya LRT line has resumed normal operations following the incident Points: On December 10,2025, the Kelana line fully resumed operationat 7.41 AM The incident was caused bya main\n",
            "\n",
            "\n",
            "[lrtkelanajaya -  - Google Chrome]\n",
            "- post_1.png\n",
            "Irt kelana jaya line current status There Was a problem; so everyone was forced to get off the train, and the entire train stopped working: The LRT (Local Transit System) information is also posted below This is a wasteland (Click to comment) something 1 O10\n",
            "\n",
            "- post_2.png\n",
            "MYLRT resumes operation Thanks to all the working people! Apower outage occurred on the Kelana Jaya LRT line between University Station and Abdullah Hukun Station causing significant disruption to passenger travel Nearly three hours after the outage; Rapid KL issued a statement on its official Faceb\n",
            "\n",
            "\n",
            "[mrt fault -  - Google Chrome]\n",
            "- post_1.png\n",
            "Eating or drinking on the subway in the US can resultin a Afriend recently asked me  eating on the Singapore subway is really punishable by a fine specifically sent this photo of myself on the subway absolutely forbiddenl The maximum fine is 500 SGD! |Vealmost never seen anyone eating Or drinking \n",
            "\n",
            "- post_3.png\n",
            "MRT took me to another ending \"It Wasn'tthat | got on the wrong train; i Was that | got on the wrong train! The same line could lead to two completely different outcomes \" Today Was my first of full-time work and my brain Was completely fried after work; all | wanted to do Was stood on the MRT plat\n",
            "\n",
            "\n",
            "[lrt is bad again - Xiaohongshu search - Google Chrome]\n",
            "- post_3.png\n",
            "MY The whole world is late My daily commute to work was disrupted by the LRT malfunction My first reaction was; \"No Way! | left home super early and it's happening againl\"  record this for the company as evidence Meat-loving shumai sauce l turns out wasn'tthe only one Who was late; forgot to record\n",
            "\n",
            "- post_4.png\n",
            "thrilling experience on the Kuala Lumpur The story happened on Christmas Eve; December 24, 2025 (I'm only just remembering to post this now so more friends can bealerted) |remember reading to the end; the last part Was really scary. That day, three friends and | Went to celebrate Christmas Eve near \n",
            "\n",
            "\n",
            "[mrt -  - Google Chrome]\n",
            "- post_9.png\n",
            "MY Learn how to take the MRT to Sunway Velocity in 30 seconds in Malaysia Edited yesterday at 22:15 in Malaysia\n",
            "\n",
            "- post_1.png\n",
            "Damansara Height MRT rental Damansara Height (Bukit Damansara) with private attached toilet Walking distance to MRT Station (Pusat Bandar Damansara). Landed house room; not condo Price: 1K If not suitable; other PJ rooms are available (large; medium, small) Inquire by WhatsApp:\n",
            "\n",
            "\n",
            "[LRT  -  - Google Chrome]\n",
            "- post_1.png\n",
            "Attention! LRT service in Malaysia is suspended! Attention LRT passengers! Plan your trip in advancel Does that mean things will return to normal after 9 O'clock? Malaysia twenty four Tangyuan Enen\n",
            "\n",
            "- post_2.png\n",
            "9/6/2025 LRT broke LRT also has Monday blues. LRT doesn't want to go to Work on Monday either Shoo-ga-di old lady Masjid Jamek was asked to get off the LRT after Waiting for more than 20 minutes Malaysla Candles pretending to be sunlight Oh my god;\n",
            "\n",
            "\n",
            "[mrt kajang -  - Google Chrome]\n",
            "- post_2.png\n",
            "Mrt kajang line MRT Kajang line to Kwasa Damansara: Malaysian citizen pass only 6 Kuala Lumpur's latest low price for must-visit spots! Everything you need for eating, drinking, and entertainmentl Connaught (Wednesday's pasar malam) Mutiara (Eko Cheras Mall & Leisure Mall), Friday night market Taman\n",
            "\n",
            "- post_3.png\n",
            "A step-by-step KL to Ukm (Metro) First; take the KJ line to the last stop on the MRT and get off at Exit A (one way RM1.8). Then take the T451 to UKM.Ifyou're only going to UKM Or PKP,get Off at the stop where you see the course (one way RMI). You can also back on where you got Off; but it will take\n",
            "\n",
            "\n",
            "[lrtkelanajaya -  - Google Chrome]\n",
            "- post_1.png\n",
            "MyKelanalayaline enters Phase 3 maintenance! Rapid KL Announces Signaling System Upgrade Work on Kelana Line; Service Affected During Hours Kuala Lumpur, November 1,2025 Rapid KL announced that the Laluan Kelana Jaya line will undergo signaling system upgrade work (Fasa 3 Phase 3) this month to furt\n",
            "\n",
            "- post_2.png\n",
            "Kelana LRT malfunction The Kelana Jaya Line experienced another malfunction during rush hour, leaving many commuters stranded at stations and disrupted. This is not the first time the line has malfunctioned at a critical time; the relevant authorities have been slow to resolve the issue; Which is di\n",
            "\n",
            "\n",
            "[lrt klcc -  - Google Chrome]\n",
            "- post_1.png\n",
            "Kuala Lumpur's money-saving king KLCC to Petaling Street MRT station for only RMZ! Petaling Street MRT: Only RM2 ~ 3.6 RMB! Don't waste RM3O ona taxi to Kuala Lumpur anymore! Take the KLCC Exit AK to Petaling Street for only RM2 3.6 RMB, compared to RM3O for a taxi a difference of 15 times! Beginner\n",
            "\n",
            "- post_2.png\n",
            "KLcc to LRT PUDU days agoin Malaysia Shuochu Weiding It should be KLCC LRT Majid Jamek internal transfer Pudu. There is no route from Pasar Seni to Pudu Malaysia really want to go back to the ordinary bUs 590. Yesterday at 13:51 Malaysia Century egg and lean pork congee (without century Go from KLC\n",
            "\n",
            "\n",
            "[mrtkajang -  - Google Chrome]\n",
            "- post_1.png\n",
            "There's a problem with the MY MRT Kajang Line The time displayed on the TV was incorrect; it showed 0 minutes when arrived at the station, but the MRT didn'tarrive for several minutes There Was a problem heading towards Kwasa Damansara; one train Edited on 01-14 Isi okay now?\n",
            "\n",
            "- post_2.png\n",
            "MRT workers Why is the Kajang MRT always so crowded and there are neverany seats morning at 7.30,1stand from Jernih to BB. It's like this going to work; and it's the same going home. Always standing to and I'm so tired February 10, Malaysia take the MRT at 5.50 am; there are plenty of seats; can ch\n",
            "\n",
            "\n",
            "[  -  - Google Chrome]\n",
            "- post_1.png\n",
            "snippets of life Life and climate in Malaysia are truly comfortable;and hourago in Malaysia This is a wasteland. (Click to comment) collect something\n",
            "\n",
            "- post_2.png\n",
            "Subway Series Episode 13 This time; we're showcasing a scene from Hang Tuah; station on the Kuala Lumpur Monorail Line 8in Malaysia 3 hours ago This is a wasteland (Click to comment) something 0 Comment\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Segment 4: Execute cleaner and print quick QA preview\n",
        "result = run_cleaner()\n",
        "print(f\"CSV output saved to: {CSV_PATH}\")\n",
        "print(f\"Cleaned output saved to: {OUTPUT_PATH}\")\n",
        "print(result[\"stats\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
